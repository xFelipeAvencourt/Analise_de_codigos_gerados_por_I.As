Inteligência Artificial (IA) generativa, isto é, um tipo de IA capaz de gerar conteúdo como humanos, vem sendo utilizada em diversas áreas, inclusive na geração automática de códigos em diversas linguagens de programação (por exemplo, C, Python, C++ e Java) fundamentados em uma descrição do objetivo do programa, a qual, se bem descrita, pode gerar resultados que atendem aos requisitos especificados pelo usuário. Entretanto, os programas gerados por aplicações de IA generativa podem conter erros lógicos ou problemas de desempenho e esse problema se torna mais evidente quando esses códigos estão relacionados a aplicações destinadas a dispositivos de rede que necessitam prover segurança (por exemplo, um \textit{firewall}). Este trabalho tem como objetivo analisar eficiências de códigos gerados por IA para problemas relacionados a Redes Programáveis (RP). Para isso, realizou-se uma revisão teórica das diferentes ferramentas de IA que seriam submetidas aos testes. A seleção foi feita com base em critérios de popularidade e de abordagem técnica, sendo selecionadas as seguintes ferramentas: ChatGPT, Copilot, DeepSeek e Blackbox.ai. A metodologia consistiu em definir três \textit{prompts} com tarefas de diferentes níveis de complexidade (simples, média e complexa) e duas técnicas de abordagens para informar os \textit{prompts (i) zero-shot e (ii) few-shot}, para verificar a capacidade de eficácia de resolução das tarefas entre cada modelo de linguagem. Em cada tarefa, iniciou-se um novo chat para definição do problema conforme a abordagem, para serem resolvidas por cada IA. O código gerado foi testado em diferentes topologias de rede simulando diferentes tipos de redes. Para a maioria das tarefas propostas as IAs implementaram soluções satisfatórias na primeira interação, no entanto, em algumas houve a necessidade de mais interações para correções
